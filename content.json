{"pages":[],"posts":[{"title":"ブログを開設","text":"ブログを開設。 モチベーション ここでは、日々蓄積される（本当に蓄積されるはずだ！）ナレッジやデータを書き留めるための、一種の備忘録、あるいは、記録として機能することになる。したがって、出来るだけサクサク書くことが出来ることが望ましい。もっと言えば、体裁の細かな微調整などに余計な神経をもっていかれたくない .. という思惑があって、もちろん、デザイン的な観点で言えば、そういうわけにもいかないことは重々理解をしている。 そうではなく、より本質にスコープしてここの記事を書いていくということが大きな目的でもある。そういうわけで、ストレスフリーにさらっと書き進めるためには、現状では Markdown 一択という選択から始めた。 いまは、CMSも多岐に渡り、高機能の恩恵を受けることが可能だが、本当は、もっとシンプルに簡単に意見を発信できれば、もっと便利なはずだ。そこで、そもそも静的なサイト（データそのもの）を記述することが出来れば、目的と合致する。 実際、世の中はある意味では原点回帰のようなことも起こっている。Wordpress や Drupal のようなかなり優秀なCMSでサイトを運営する方向と、インターネット黎明期のような静的サイトへの回帰。後者は非常におもしろい分野へと変化してきている。これは、ある意味、Jam Stack分野への回帰としても捉えることが出来るだろう。たとえば、データベースがバックエンドに存在しなければ、そもそもセキュリティ面でのアドバンテージを稼ぐことが、非常にシンプルになり得る。同時に、更新の手間暇の煩雑さからの開放という点でも大きな効果が得られる。という動機付けのもとに、この備忘録は、Hexoというパッケージでサイトを構築し、その結果をGithubでホストしている。ブログは、Githubの中の GitPages でホストされるというしくみをとっている。 執筆環境は Visual Studio Code から、HexoパッケージをインストールしたLinuxサーバへSSHで接続してMarkdownを快適に書き進めることが出来るようなイメージ。 プラットホーム自体は、Node.js があれば、さくっと立ち上がるものでもあるが、そのしくみも含めて構築手順書的な観点で、あるいは、備忘録という意味で、本サイトを公開するまでの流れを書いておく。 Quick Start1、hexo環境の構築 すでにNode環境があれば別だが、今回は Docker でオートメーションを達成したい。あるいは、地道に環境を構築してもまったく構わない。その際、OSは Windows, Mac, Linux を問わない。Nodeの実行環境が手に入ればプラットホームは問わない。 以下に、Dockerfile の内容を示す。 1234567891011121314151617181920212223242526272829303132333435363738394041FROM ubuntu:16.04# proxy ここは必要に応じて#ENV https_proxy=\"http://name:password@proxy:8080/\"#ENV http_proxy=\"http://name:password@proxy:8080/\"# OpenSSHRUN apt-get update &amp;&amp; apt-get install -y \\ openssh-serverRUN mkdir /var/run/sshdRUN echo 'root:root' | chpasswdRUN sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config# SSH ログインRUN sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshdENV NOTVISIBLE \"in users profile\"RUN echo \"export VISIBLE=now\" &gt;&gt; /etc/profile# 一般ユーザを作成して、Node.jsのリポジトリからステーブルを選択してインストールRUN apt-get install -y git less vim sudoRUN apt-get install -y nodejs npmRUN npm install n -gRUN n stableRUN apt purge -y nodejs npmRUN n 11.2.0RUN useradd -m aandeRUN echo 'aande:aande' | chpasswdRUN usermod -aG sudo aandeRUN chsh -s /bin/bash aande# HexoRUN npm install -g \\ hexo-cliRUN npm install \\ hexo-deployer-git --save \\ hexo-admin --save# SSH、Webサーバ、Adminサーバ公開用のポートEXPOSE 22 4000 8080CMD [\"/usr/sbin/sshd\", \"-D\"] 今回は、OSに Ubuntu16.04 を指定した。簡単に解説を加えると、はじめのレイヤーでOpenSSHのインストールとroot権限を設定している。 Dockerの特性として何層かのレイヤーでビルドを積むことにあるので、細かく（あまり細かすぎても、あるいは、薄すぎても別の弊害を生みが …）層を重ねることによりビルド時間の短縮効果を得ることが出来る。次に、Node.jsをセットアップしている。ここでインストールしている Node.js, npm の目的は、パッケージマネージャを通じて n package を導入することにある。Windowsで言えば nodist のような役割で、これによりどのバージョンの Node.js をインストールして使用するか、を選択可能としている。 事前の調査で現在の Node.js の Stable/LTS を確認して、バージョンを選択してインストールしている。ここまでのコードの実行後は、目的の Node.js がインストールされたので、事前にインストールした（つまり、n package を導入することを目的とした）バージョンは削除している。もちろん削除しなくても構わない。とくに環境汚染（実際に汚染があるわけではないが）の類の問題に関してナーバスな方は、きれいしに整理したほうがよいことは言うまでもない。 ここで、n package の説明を簡単に。別のバージョンの Node.js のインストール、管理がシンプルになるので、そういった目的の場合は、以下のコマンドを実行する。 1234567891011$ sudo n 10.12.0$ node -vv10.12.0# 上記のコマンド実行により、バージョン 10.12.0 に切り替わった。 # また、エイリアスも使用できる。# 下記は、LTSの選択。$ sudo n lts$ node -vv10.15.3 さて、話が横道に逸れてしまったが、元に戻す。次に一般ユーザの設定をして、次が本筋の Hexo のインストール。hexo-cli をグローバルインストールをして、続いて Githubデプロイ用に必要な hexo-deployer-git をインストールしている。もちろん、gitコマンドでデプロイ（というか、Push）するのであれば、それでも構わない。次にセットアップする hexo-admin は、ブログの作成をブラウザ上から実行出来るようにするための、いわゆるダッシュボードのようなもので、無理にインストールする必要はない。今回も、記事の作成は、Visual Studio Code でサクサク … というのが主目的なので、使用する予定はない。ただ、一応、関連パッケージということで紹介をしてみた。 最後にポートを export しているが、これは言うまでもなく、SSH用に22番、ブログ表示を4000番（つまり、デバッグ用の … というか、ローカル表示確認用のWebサーバ役と、前段で説明したダッシュボード表示のために8080番を開けている。Dockerfileに関する説明は以上。 次いで、上記の Dockerfile に基づいて、Dockerイメージをビルドする。イメージの名前は任意。 1$ docker build -t ssh-node-sandbox:latest . 出来上がったイメージからコンテナを生成する。 123$ docker run -d \\ -p 20022:22 -p 4000:4000 -p 8080:8080 \\ --name ssh-hexo-blogger ssh-node-sandbox これで dockerコンテナが起動した。実行されているか見てみよう。 123$ docker psb3436e362b47 ssh-node-sandbox &quot;/usr/sbin/sshd -D&quot; 26 hours ago Up 2 hours 0.0.0.0:4000-&gt;4000/tcp, 0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:20022-&gt;22/tcp ssh-hexo-blogger OK！ では、さっそくコンテナにSSHで入ってブログサイトを構築してみるが、その前に Visual Studio Code にSSHの拡張プラグインをセットアップする。もちろん、ほかのSSHクライアントで接続してLinux上のVimなどのエディタを使用しても構わないが、ここは、Visual Studio Code の快適さを選択したい。選択したのは SSH FS を使用する。 2、ブログサイトの構築コンテナに入ったら、以下の手続きでブログサイトを構築する。 12345678910# 任意のディレクトリに移動して、そこをワークスペースとする。# ここから Hexoコマンドを実行して、ブログサイトを構築する。$ hexo init blog# 最後の blog がサイト名となる。もちろん任意でOK。# 作成したサイト用ディレクトリに移動する。$ cd blog# ここで、ディレクトリ内のオーダーを見ると package.json があるはずなので、その中にリストされているライブラリを導入する。やり方は簡単だ。以下の1行のコマンドを実行するのみ。$ npm install これで、ブログサイトの雛形が展開された。さっそく記事を書いてみる。 3、記事の作成1$ hexo new &quot;記事の名前&quot; 上の記事の名前は任意。ページのテンプレートが作成されたので、おもむろに記事を書く。記事を書き上げたら、以下のコマンドでサーバを起動する。 1$ hexo server これで、ローカル環境で書いた記事を確認出来る。ブラウザで localhost:4000 にアクセスする。いま、書き上げた記事が表示されるはずだ。dockerのホストに docker-machine を使用している場合は、おそらく、192.168.99.100:4000 のようなアドレスになるはずなので、ここはケアレスミスのポイント。慌てずに確認。この部分は、dockerのホストが何であれ、WindowsもMac, あるいは Linux でも変わりはない。 どうだろうか？これで簡単にブログサイトを、ローカル環境ではあるが構築出来てしまった。これだけでも簡単に目的が達成できたことが実感出来るだろう。しかし、ここまでが前段で、残りのデプロイがある。 4、デプロイ最後のプロセスまで辿り着いた。まずは、以下のコマンドを初期に覚えておくとよいかもしれない。 はじめは、以下の generate コマンド。 123$ hexo generate# または$ hexo g これは公開用のリソースを、asset も含めて publicフォルダ に生成するためのコマンド。 つまり、HTMLコードや公開に必要な画像などのファイル群、あるいは css なども含めて、出力される。このHexoソリューションのディレクトリ構成を見ると、いま作成中のドキュメントは source 配下のサブフォルダに存在するはずだ。このソースがビルドされた後、publicフォルダに公開用の静的セットとして生成されるというしくみだ。上記の generate コマンドの実行をすると、コンソールにその結果が出力されるので確認は容易だ。 さて、最後のコマンド。deploy。 123$ hexo deploy# または$ hexo d 今回想定している Github リポジトリに公開用ファイルがデプロイされる。そして、Github の当該リポジトリにアクセスをして、Settingsタブを選択する。その後、画面を下のほうにスクロールして、Github Pages の項目を見つければ、そこにたったいまデプロイしたブログのURLが設定されている。そこをクリックすると、結果を表示出来る。これですべてが完了だ。 ここまでの作業を長々と説明を加えてきたが、手順をしっかり踏めば、おそらく10分程度でサイト公開までは辿り着けるはずだ。もちろん記事を書く時間は除外する。つまり、記事を書くのが本質であって、それ以外にはなるべくサクッとすませたいというのが今回の狙いでした。 Best Regards; 今週気になったニュース■ PLCの未来像を描く「PLCnext」、用途別アプリなどを順調に拡大 - ハノーバーメッセ2019ラダー言語を用いることが多い PLC だが、Visual Studio、Eclipse、MATLAB/Simulink、PC Worx Engineerなどのツールの活用に加え、C#などの活用も進みそうだ。かつ、オープンプラットフォームで制御領域の拡大にも一気に貢献しそうな気配でもある。","link":"/blogger/2019/04/20/2019-04-3rdweek/"},{"title":"Jupyter notebook のインストール","text":"近年、python が基軸言語 … とまでは言わないが、その重要性は確固たる地位を築きつつあるのと同時に、興味は持っているが、あるいは、始めてみたいとは思っているが、なかなか手がつけられないという相談をあちこちで受ける機会が多くなってきた。 そういうわけで今回は、python を手軽に試してみたい、という向きに、python の環境構築の方法を紹介したい。それも出来るだけシンプルに手軽に、というポイントに絞り記事を書いてみたいと思う。と言うのも、python の処理系を構築するには、ネットで調べてみると、多くの情報を集めることが出来るが、あまりにも多くの情報が見つかり、体系的に整理するのが逆に困難であることが災いして手軽に試すということから遠ざかってしまう場合が多いように思われるからだ。 たとえば、まず python の最新版をインストールした後、virtualenv を導入して、目的のバージョンの python を個別にインストールして仮想ディレクトリに封じ込めて環境汚染を防ぐ … というような記事をそのままトレースしようとすると、また今度にしよう、と考えるのも無理はないと思う。なので、たったいまここで書いた内容は、一度頭の中から除去して、とにかく python のコードを何行か書いて（それも即時性のある、意味のある、手軽に目的を達成するような）試すことができる記事を目指したいと思う。 まずは、ここで python をインストールするための方針を固める。anacondaパッケージを導入して、pythonの処理系をインストールした後、jupyter notebook で python のコードを実行する、という方針で進めたい。 今回のゴール python の実行環境である Anaconda のインストール jupyter notebook で python のコードを実行 さて、ここまで読んでいただいて、はて？ python のインストールという文脈で、jupyter notebook とは、いったい何なんだ？ という疑問を持たれた方も、当然多いと思う。ここだけは、ひとまずガマンして頂いて、python をインストールするにあたって、jupyter notebook というものが出てくるのか・・ 程度に理解してもらえれば充分だ。強いて言えば、python を実行するためのインタープリタを持つツールという位置づけで理解してもらえればよいと思う。とにかく python のコードを実行した後、理屈を理解していけば入り口としては単純だ。 今回は最もシンプルに試すために、python環境を構築するOSとして Windows を想定する。 早速 ここにアクセスして 画面右上にある Download をクリックする 画面中間部にあるWindowsのアイコンをクリックする 画面下にたどって、Python 3.7 Version Download と書かれた下に、64-Bit、または、32-Bitの都合の良い方のインストーラをダウンロードする これで、pythonの動作環境のインストーラが手に入ったはずだ。少し解説を加えておく。たったいまダウンロードしたインストーラは、pythonの動作環境に加えて、さまざまなライブラリ、ツールを含んだ anaconda と呼ばれるパッケージのインストーラを手に入れたことになる。詳しくは省略するが、これをOS上に展開することにより、pythonの具体的、かつ、興味深い一面をいとも簡単に実行に移すことが可能になる。 では、インストーラをダブルクリックしてインストールしてみよう。 2019年3月現在の最新バージョンであることがわかる。Next をクリックして次に進もう。 ライセンス条項を確認して I Agree をクリック。 クレデンシャルの設定だが、自分だけで構わないと思われるので、Just Me をチェックして Next をクリック。 インストールするディレクトリは任意で構わない。値を設定したら、Next をクリック。 ここは、Register Anaconda as my default Python 3.7 のほうにチェックを入れて、Install をクリック。ここでインストールが始まるので、その間にコーヒーでも淹れてリラックス。 さて、インストールが完了したら … Quick Startコマンドプロンプトを起動して、以下のコマンドを入力後 Enter をヒット。 &gt; jupyter notebook その後、ブラウザで http://localhost:8888 にアクセス。これで、jupyter notebook が起動した。これでひとつのハードルは越えた。おめでとう。 次に、画面右側にある New というプルダウンメニューが見えると思う。そこを選択して、表示されたメニューの中から Python 3 を選択すると、notebook が表示されると思う。まさにこれが、これから python のコードを実行するにあたりさまざまなコードを入力するノートブックだと思って欲しい。jupyter notebook は、pythonコードの実行を、このようにあたかもノートブックに記録するように保存し、ロードし、頭の中の試行錯誤を具現化するための抽象インターフェースだと考えればいいと思う。 使い方だが、簡単な説明だけを書く。In [ ]: の右側に細長いボックスがあるが、これがセルと呼ばれる項目だ。この部分にコードを入力して実行すると Out[ ]： という形で結果が得られる。それだけだ。どうだろう。簡単じゃないか。ひとつ簡単な例で試してみよう。In[ ]: の右側のセルに、次のようなコードを入力して、Shiftキーを押しながら Enter をヒットしてみよう。 In [ ]: x = 2 x * 2 Out [ ]: 4 上のような結果が得られれば OK。これは、つまり、x という名前の変数を用意して 2 を代入している。次に、x * 2 を実行する。その結果 4 が得られた。これで、python のコードが実行されたことになる。素晴らしい！しかし、これでは、あまり実用的ではないし掘り下げていくにはあまりにも単純すぎる。そこで、次はもう少し実用的で、かつ、高度でありながら python らしいシンプルな解法を試してみることにする。それでは、例を下に示すが、サンプルコードを実行するにあたり、少し辛抱して、下記のコマンドを実行して欲しい。なお、コマンドの実行にあたり、コマンドプロンプトをもうひとつ新たに立ち上げて実行すること。なぜなら、さきほど jupyter notebook を起動したDOSプロンプトはフォアグラウンドでpythonのプロセスが実行されているからだ。jupyter notebook のログが出力されているのが観察出来ると思う。したがって、別のコマンドプロンプトを起動して下記のコマンドを実行して欲しい。実行するディレクトリは任意で構わない。 &gt; pip install pandas-datareader いまの段階ではこのコマンドの意味をわからなくても構わないので、とにかく実行してエラーがなく終了することを確認する。ちなみに、いま実行したコマンドは python の処理系に pandas-datareader というライブラリを導入する役割を実行したことになる。では、pandas-datareader とは何か？ と言うと、簡単に言えば日経平均株価を取得するものだ。 次に、コード例を示す。さきほどと同じように、jupyter notebook 上で、新しいノートを新規作成して入力する。やり方は、New というプルダウンメニューで Python 3 を選択する。すると新しいノートが作成されるので、In [ ]: の右側にあるセルに次のコードを入力する。 In [ ]: import datetime import requests import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import pandas_datareader.data as web 上記のコードの意味は、これから実行するコードのライブラリをimportすることにある。いちばん下のコードが、さきほど導入した pandas-datareader をimportしているコードで、その他は anacondaパッケージ に備わっている標準ライブラリをimportしているコードだ。ここまで入力したら、Shiftキーを押しながら Enterキーをヒット。続いて … In [ ]: start = datetime.datetime(1970, 1, 4) end = datetime.datetime(2019, 4, 26) # 必要に応じて設定 InsecureRequestWarning session = requests.Session() session.verify = False df = web.DataReader(\"^N225\", \"yahoo\", start, end, session=session) 上記のコードは、日経平均株価のデータを取得しているが、当該セクション冒頭で1970年1月4日から今年の4月26日までのデータを取得する設定をしている。 In [ ]: # indexを日付型に変換して、Columnのデータを取り出す ... date = pd.to_datetime(df.index) series = df['Close'] # プロットする。 plt.figure(figsize=(6,4)); plt.plot_date(date, series, '-'); 上記では、インデックスを日付型に変換して、時系列にデータが並ぶように設定した後、日経平均の終値を取り出しグラフにプロットしている。その結果、出力されたグラフが下記のとおり。 ちょうど、80年代も終わろうとしているあたりから90年代のはじめにかけてピークを迎えているのが見てとれるだろう。いわゆるバブル期である。2009年ごろに底を打ち、その後徐々に上方基調になろうとしているように見えるが、本当にそうだろうか？ それはもっと多角的な統計解析を実行しないと、その判断はおそらく難しいと思われる。 このように python を使えば、思ったより簡単に情報の可視化が実行できることをわかって頂けたことだろうと思う。ここから先は、ぜひ、皆さんのセンスと実行力で掘り下げてほしいと思う。なお、今回、インストールを試みた anacondaパッケージには、統計分析、数学的処理に関するライブラリが標準で備わっているので、そのおもしろさをシンプルに体験することが可能だ。 まとめ 現在 python の最新版は、バージョン 3.7 及び 2.7 の2つの系統が存在している。どちらの選択肢をとればよいのか？ に関しては、現時点では 3.7 にすること。それ以上は、今後掘り下げた場合に必要に応じてメンテナンス体系を把握していけばよい。 pythonをインストールするにあたり、anacondaパッケージをとくに推奨する。理由は、python周辺の体系や文化になれるまで悩む箇所を極力減らすためだ。 pip install というコマンドは、anacondaパッケージに収録されていないライブラリを導入するための、いわゆるパッケージマネージャである。 今回の記事では、pythonのコードやセンテンスに関してほとんど説明をしていない。それは、即効性のあるコードをまず実行してみて結果を得てみる … ということを重視したためだ。今回のコードを徐々に編集しつつ疑問を持った時点で調べるほうが興味を維持しやすいと思われるし、とくに pythonは、いろいろな情報にあふれているので、可視化という観点で試してみて、いろいろ調べてみる、というアプローチをとったほうが手を動かしやすいだろう .. という判断でもある。 今回は、ここまで。 Best Regards;","link":"/blogger/2019/05/06/2019-05-1stweek/"},{"title":"Cython と pure Python 実行速度比較","text":"今回は、Cython と pure Python の比較の実験をしてみたいと思う。Pyhonはとにかく遅いと言われる。型を持たないスクリプト言語なのだから仕方ないとは思うが、それでも遅いと言われる。そんな中でこの課題を解決しようとする文脈でよく登場するのが Cython ではあるが、では、どのくらいの差が出るのか？ そもそもパフォーマンスの違いは劇的なのか？ を実際に検証してみたいと思う。 そもそも Cython とはいかなるものなのか？ を簡単に説明しておきたいと思う。 Cythonは、低級言語である（C/C++）にコンパイル（つまりネイティブコードを生成して）パフォーマンスの課題に対して答えを出すというところに狙いがある。呼び出しは Python のメカニズムによるので、つまり、それはマジックコマンドを通じて、コンパイルされたコードが実行される、ということだ。しかしながら、ほとんど Pythonコードに遜色のない状態の記述から呼び出したり、あるいは、しっかりと型指定を定義した状態で実行したり、と、とくにパイソニックなライブラリとのコンビネーションを企てようとすると一筋縄ではいかない側面を当然持っている。 ここでは、その上澄みをとりつつも、ピュアなPythonとネイティブではこのくらいの差が出るのか、あるいは、そうでもないのか、という部分をちらっと垣間見てみたいと思う。 それでは、テスト環境を構築する。ふだん使用している環境には手を入れずに、テストをするだけの目的で環境を立ち上げて検証が終わればさっさと処分するのみの環境なので、ここから書くことはあまり本質的ではないので、先を急ぐ方は、もう少し後ろまで読み飛ばして大丈夫です。 今回のテストは このイメージ を使用して dockerコンテナを立ち上げる。このイメージは、Anacondaで構築されているので、コンテナを作成すればそのままJupyter Notebookをすぐに使用可能。 Quick Start コンテナで環境を作成では、Docker Hub よりイメージをダウンロードする。1$ docker pull jupyter/datascience-notebook コンテナを起動する前に同梱されている Python関係のバージョンをチェックする。1234567891011$ docker run --rm jupyter/datascience-notebook python --versionPython 3.7.3$ docker run --rm jupyter/datascience-notebook jupyter --version4.4.0docker run --rm --name test jupyter/datascience-notebook jupyter notebook --version5.7.8docker run --rm --name test jupyter/datascience-notebook cython --versionCython version 0.29.6 自身の環境ではこうなった。Cythonもちゃんと入っている。したがって、このコンテナを起動すればそのままテストが実行できる。では、起動する。 1$ docker run -it --rm -p 8888:8888 jupyter/datascience-notebook これでOK。ここで注意しないといけないのは、–rm コマンドを発行しているので、コンテナをシャットダウンしたら、すべて削除される。つまり、テスト結果もなくなるので、ご注意を。逆に言えば、コンテナを落とせばテスト環境もきれいに片付く、ということだ。起動後、ブラウザで localhost:8888 にアクセスすれば Jupyter Notebook が起動する。docker-machine を使用されてる方であれば、おそらく 192.168.99.100/101:8888 みたいなアドレスになると思う。Jupyter Notebookを開くことが出来たら トークンの入力を求められると思うので、ログを読みだす必要がある。以下のようなコマンドで標準出力にログが書き出されるので、そこからトークンを読み取って欲しい。 1$ docker logs &lt;コンテナID&gt; コンテナIDは、以下のコマンドで確認できる。1$ docker ps では、早速 notebookを新規作成して … 以下のコードを入力する。 Notebook1%load_ext Cython ここから4種類の条件を違えて、コードを記述してゆく。まずは、pure Pythonで記載したコード Notebook12345def pure_python(n): a, b = 0, 1 for i in range(n): a, b = a + b, a return a pure Python での実行速度を計測した結果は以下のとおり。 Notebook12%timeit pure_python(5000)519 µs ± 5.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 次に、Cythonで記載したコード Notebook1234567%%cython def cython(n): a, b = 0, 1 for i in range(n): a, b = a + b, a return a そして、実行結果は …Notebook12%timeit cython(5000)419 µs ± 1.58 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) かすかに速くなってはいるという感じ。次に、引数に型アノテーションを付与したCythonコード Notebook1234567%%cython def cython_typed_anotation(int n): a, b = 0, 1 for i in range(n): a, b = a + b, a return a 実行結果は …Notebook1%timeit cython_typed_anotation359 µs ± 977 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each) また、かすかに速くなっていることが確認できるが、このコードブロック単体ではそれほど大きな効果を実感できるというイメージではない。最後に、関数内の変数すべての型アノテーションを付与した CythonコードNotebook123456789%%cython def cython_alltyped_anotetion(int n): cdef int a,b,i a, b = 0, 1 for i in range(n): a, b = a + b, a return a 実行結果は …Notebook12.84 µs ± 71.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) ここにきて大きな変化があった。わかりにくいので結果を下記にまとめる。pure Python 519 µs ± 5.93 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)Cython 419 µs ± 1.58 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)Cython 引数に型指定 359 µs ± 977 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)Cython すべての引数に型指定 2.84 µs ± 71.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) ここまでやるか … という内容ではあるが、すべての引数に型指定をした場合のインパクトはかなり大きいということがわかった。可読性を犠牲にするのに加えて、あまりパイソニックな感じがしないコードになってしまうが、これはこれで効果があるということがデータとしてわかった。では、厳密に型指定を施したCythonコードとは他の条件、つまり、引数にのみ型指定をした場合と型指定をしない場合のCythonコードに関して効果は見込めないか？ というと、おそらく単体のコードブロックのみを使用するのであればほとんど意味はないと思われる。どのように使われる関数であるか？ という観点での踏み込みで判断すべきことだろう。Pythonには Numpy という優れたライブラリがあるので、これで達成できないようなあまり合理的ではないコードが、もしあった場合は、考える余地はあるかもしれない。 今度は、もう少し具体性のあるコードブロックを使ってその差を検証してみようと思う。画像解析をイメージして、2次元配列に対する離散畳み込みのフィルター処理で比較してみたいと思う。想定としては、画像を表現するNumpyの二次元配列と畳み込みのカーネルを引数で与える関数を書いて条件をふって比較する形だ。画像サイズは100X100、カーネルは5X5 の条件。 Numpyをimportしておく。Notebook1import numpy as np そして、まずは pure なコードブロックを想定した関数。Notebook123456789101112131415161718192021222324252627def pure_filter(f, g): vupper = img.shape[0] wupper = img.shape[1] supper = filter.shape[0] tupper = filter.shape[1] smidium = supper tmidium = tupper xupper = vupper + 2*smidium yupper = wupper + 2*tmidium # 出力画像を想定した配列 h = np.zeros([xupper, yupper], dtype=f.dtype) # 畳み込み処理 for x in range(xupper): for y in range(yupper): # filter の各ピクセルに対する加算 s_from = max(smidum - x, -smidum) s_to = min((xupper - x) - smidum, smidum + 1) t_from = max(tmidum - y, -tmidum) t_to = min((ymidum - y) - tmidum, tmidum + 1) value = 0 for s in range(s_from, s_to): for t in range(t_from, t_to): v = x - smidum + s w = y - tmidum + t value += filter[smidum - s, tmidum - t] * img[v, w] h[x, y] = value return h そして、実行してみる。Notebook1234N = 100img = np.arange(N*N, dtype=np.int).reshape((N,N))filter = np.arange(81, dtype=np.int).reshape((9, 9))%timeit -n2 -r3 pure_filer(img, filter) 結果は …Notebook1414 ms ± 1.57 ms per loop (mean ± std. dev. of 3 runs, 2 loops each) 今度は、Cythonのアノテーションを意識したコードブロックを書いて実行してみる。条件は同じ画像、同じカーネルフィルタの想定。Notebook123456789101112131415161718192021222324252627282930313233%%cythonimport numpy as npcimport numpy as npDTYPE = np.intctypedef np.int_t DTYPE_tdef cython_filter(np.ndarray img, np.ndarray filter): cdef int vupper = img.shape[0] cdef int wupper = img.shape[1] cdef int supper = filter.shape[0] cdef int tupper = filter.shape[1] cdef int smidium = supper // 2 cdef int tmidium = tupper // 2 cdef int xupper = vupper + 2*smidium cdef int yupper = wupper + 2*tmidium cdef np.ndarray h = np.zeros([xupper, yupper], dtype=DTYPE) cdef int x, y, s, t, v, w cdef int s_from, s_to, t_from, t_to cdef DTYPE_t value for x in range(xupper): for y in range(yupper): s_from = max(smidium - x, -smidium) s_to = min((xupper - x) - smidium, smidium + 1) t_from = max(tmidium - y, -tmidium) t_to = min((yupper - y) - tmidium, tmidium + 1) value = 0 for s in range(s_from, s_to): for t in range(t_from, t_to): v = x - smidium + s w = y - tmidium + t value += filter[smidium - s, tmidium - t] * img[v, w] h[x, y] = value return h 結果は …Notebook123456N = 100f = np.arange(N*N, dtype=np.int).reshape((N,N))g = np.arange(81, dtype=np.int).reshape((9, 9))%timeit -n2 -r3 cython_filter(f, g)372 ms ± 301 µs per loop (mean ± std. dev. of 3 runs, 2 loops each) 並べてみる。pure Python 414 ms ± 1.57 ms per loop (mean ± std. dev. of 3 runs, 2 loops each)Cython すべての型に型指定 372 ms ± 301 µs per loop (mean ± std. dev. of 3 runs, 2 loops each) あまり効果は出ていないということがわかった。とはいえ、かなり素直なコードであるのでこういう答えが出たが、アルゴリズムの工夫ではもう少し詰められるのかどうか … は、検討課題だと思われる。しかし、このぐらいであれば素直に OpenCV のほうがシンプルではあるだろう。 今週気になったニュースVisual Stuio 2019先月のニュースにはなってしまうが、正式版になりました。今回は、Github上のコードを学習した結果賢くなった IntelliCode と、ペアプログラミングを支援する Live Share が正式版になったということが大きな目玉ではあるが、地味なところでは、メモリの使用量をぐっと抑えて 1/4 にしたというところがある。 Google I/O 2019 Kotlinファースト 強化を表明この時期になると Google I/O ではあるが、現在、Android開発者の50％以上がKotlinユーザだそうだ。 Microsoft Build 2019 - WSL 2同じく5月の Microsoft Build ではあるが、個人的にもっともぐっときたのがこのアナウンスでした。Windows Subsystem for Linux 2 となり、Linuxシステムコールのフル互換をうたっています。しかも、正式に Docker対応をアナウンスしました。仮想マシーンを用意することが大きな壁になっていた感はありますが、これは Docker にチャレンジする大きな機会かもしれませんよ！ Best Regards;","link":"/blogger/2019/05/10/2019-05-2ndweek/"}],"tags":[{"name":"jupyter","slug":"jupyter","link":"/blogger/tags/jupyter/"},{"name":"Cython, Docker","slug":"Cython-Docker","link":"/blogger/tags/Cython-Docker/"},{"name":"Cython","slug":"Cython","link":"/blogger/tags/Cython/"},{"name":"Docker","slug":"Docker","link":"/blogger/tags/Docker/"}],"categories":[]}